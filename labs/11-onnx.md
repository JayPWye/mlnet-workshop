# Phase 11: ONNX

In this session, you will learn how to use an object detection ONNX model from Azure Custom Vision inside a .NET application.

## What is Azure Custom Vision?

Azure Custom Vision is a cloud-based service that lets you build, deploy, and export custom deep learning computer vision models. Visit the [Azure Custom Vision](https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/home) documentation to learn more.

## What is ONNX?

The Open Neural Network Exchange (ONNX) is an open source format for AI models. ONNX supports interoperability between frameworks. This means you can train a model in one of the many popular machine learning frameworks like PyTorch, convert it into ONNX format and consume the ONNX model in a different framework like ML.NET. To learn more, visit the [ONNX website](https://onnx.ai/).

![ONNX Description](https://user-images.githubusercontent.com/46974588/88130578-eaa75300-cba8-11ea-9da6-b1fd90169f8a.png)

## Phase 11.1: Inspect the model

In this section, you will learn how to visualize an ONNX model using Netron.

Normally, you would need to train a model and export it to ONNX. For this lab, you already have a pretrained ONNX model in the `models` directory in this repository. The model was trained using Azure Custom Vision. The task is object detection and the export format was Compact(S1). Visit the Azure Custom Vision documentation to learn more about [exporting ONNX models](https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/export-your-model).

In your browser, open [Netron](https://lutzroeder.github.io/netron/).

In Netron, select **Open Model...** and use the file explorer to import the `model.onnx` model.

A graph appears showing the layers and nodes of your model. Nodes have annotations and metadata which describes the model such as the model format, a short description, inputs and outputs. 

![Netron Azure Custom Vision Object Detection](https://user-images.githubusercontent.com/46974588/88125411-3c49e080-cb9d-11ea-80d8-661bc85cf594.png)

Select the first node in the graph, labeled **data**.

The input for the model is the following:

- *data*: A `float` array of size 3 x 320 x 320 containing the RGB values of a 320px x 320px image.

The output generated by the model contains the following:

    ![ONNX Azure Custom Vision Outputs](https://user-images.githubusercontent.com/46974588/88129943-525c9e80-cba7-11ea-986d-016cee6b9b9b.png)

- *detected_boxes*: a `float` array of size `n x 4` where `n` is the number of bounding boxes found in the image. The 4 represents the coordinates of two points P1(x1,y1) and P2(x2,y2) for each bounding box found by the model. The first point corresponds to the top-left point of the bounding box and the second point corresponds to the bottom-right point of the bounding box. 

    ![ONNX Azure Custom Vision Detected Boxes](https://user-images.githubusercontent.com/46974588/88129961-57b9e900-cba7-11ea-9b3d-5339336ad851.png)

- *detected_scores*: a `float` array of size `n` where `n` is the number of bounding boxes found in the image. The values range between 0 and 1.
- *dectected_classes*: a `float` array of size `n` where `n` is the number of bounding boxes found in the image. The model used in this sample detects two classes:
  - 0: CrackedWindshield
  - 1: Dent

## 11.2: Define model input schema

In this section you install the required NuGet packages and define the model input schema.

First, we need to add a few NuGet packages to the `ONNXConsole` project.

If you're using Visual Studio, right click on the project name and select **Manage NuGet Dependencies**. Then click the "Browse" tab and search for `Microsoft.ML.OnnxTransformer`. Make sure to install version **1.5.1**.

![Install Microsoft.ML.OnnxTransformer NuGet package](https://user-images.githubusercontent.com/46974588/88380171-0c076b00-cd72-11ea-8f05-631612cea9b9.png)

Repeat these steps for the `Microsoft.ML.OnnxRuntime` version `1.4.0` and `Microsoft.ML.ImageAnalytics` version `1.5.1`.

Alternately if you prefer working from the command line, you can run this command from the *src/ONNXConsole* folder:

```dotnetcli
dotnet add package Microsoft.ML.OnnxTransformer -v 1.5.1
dotnet add package Microsoft.ML.OnnxRuntime -v 1.4.0
dotnet add package Microsoft.ML.ImageAnalytics -v 1.5.1
```

### Define model input schema

In the `Shared` project, add a new class called `ONNXInput` to the root directory.

Then, define the class as follows:

```csharp
public class ONNXInput
{
    public string ImagePath { get; set; }
}
```

![Define ONNX input schema](https://user-images.githubusercontent.com/46974588/88380667-f5154880-cd72-11ea-8931-9a7aae0119d9.png)

## 11.3: Define ML.NET ONNX scoring pipeline

In this section, you will learn how to use an ONNX model for scoring inside of an ML.NET pipeline.

Open the `Program.cs` file in the `ONNXConsole` project.

Add the following `using` statements at the top of the file:

```csharp
using Microsoft.ML;
using Shared;
```

Then, in the class definition, add the location where the model is saved.

```csharp
// Update this with the location where you saved the model
private static string ONNX_MODEL_FILEPATH = @"C:\Dev\mlnet-workshop\models\model.onnx";
```

Inside the `Main` method, create an instance of `MLContext`.

```csharp
MLContext mlContext = new MLContext();
```

Create a pipeline to load an image and uses the ONNX model to detect objects in the image by adding the following code:

```csharp
var pipeline =
    mlContext.Transforms.LoadImages("Image", null, "ImagePath")
    .Append(mlContext.Transforms.ResizeImages("ResizedImage", 320, 320, "Image"))
    .Append(mlContext.Transforms.ExtractPixels("data", "ResizedImage"))
    .Append(mlContext.Transforms.ApplyOnnxModel(
        outputColumnNames: new string[] { "detected_boxes", "detected_scores", "detected_classes" },
        inputColumnNames: new string[] { "data" },
        modelFile: ONNX_MODEL_FILEPATH));
```

This pipeline performs the following steps:

- `LoadImages` takes the path of an image and creates a Bitmap in the `Image` column.
- The model expects a 320px x 320px image. Therefore, the contents of the `Image` column are resized and stored in the `ResizedImage` column.
- Once the image is resized, extract the pixels into a column called `data` using the `ExtractPixels` transform. Notice that the name of the column containing the pixels is `data` because that's the entrypoint of the model.
- Finally, the model is used to detect objects by using `ApplyOnnxModel` transform with the respective input and output columns of the model provided.

![Define ONNX object detection pipeline](https://user-images.githubusercontent.com/46974588/88381735-f5aede80-cd74-11ea-8db2-6015bc447416.png)

## 11.4: Build and save ML.NET ONNX scoring pipeline

In this section you will learn how to build and save an ML.NET scoring pipeline that uses an ONNX model.

Now that you have the pipeline, you can create an ML.NET model that contains all of the preprocessing and scoring transforms. 

Since no training is actually taking place, you can use an empty `IDataView` when building the pipeline.

Below your pipeline definition, create an empty IDataView and call `Fit` to create the model.

```csharp
// Build the pipeline
var emptyDV = mlContext.Data.LoadFromEnumerable(new ONNXInput[] { });
var model = pipeline.Fit(emptyDV);
```

Now that you have a pipeline, save it for later use. The model is serialized and stored as a `.zip` file. In this case, the model will be saved to a file called *ONNXModel.zip*.

Inside the top of the class definition, set the path where you want to save your pipeline to.

```csharp
private static string PIPELINE_FILEPATH = @"C:\Dev\ONNXModel.zip";
```

Then, at the bottom of the `Main` method, add the following code:

```csharp
// Save the pipeline
Console.WriteLine("Saving pipeline...");
mlContext.Model.Save(model, emptyDV.Schema, PIPELINE_FILEPATH);
```

![Save ML.NET ONNX Scoring Pipeline](https://user-images.githubusercontent.com/46974588/88382256-162b6880-cd76-11ea-83b2-21bb5664af4a.png)

Set the startup project to `ONNXConsole` and run the application.

Congratulations! You have now built and saved a prediction pipeline that uses an ONNX model.

## 11.4: Consume the model

In this section, you will learn how to take a saved ML.NET ONNX scoring pipeline and use it in a Razor Pages web application to detect car damage.

At this point, you have taken an ONNX Model from Azure Custom Vision and used it for scoring as part of an ML.NET pipeline. You can take that saved pipeline and use it inside of the web application to detect damage and adjust prices accordingly.

### Install NuGet packages

First, we need to add a few NuGet packages to the `Web` project.

If you're using Visual Studio, right click on the project name and select **Manage NuGet Dependencies**. Then click the "Browse" tab and search for `Microsoft.ML.OnnxTransformer`. Make sure to install version **1.5.1**.

![Install Microsoft.ML.OnnxTransformer Web](https://user-images.githubusercontent.com/46974588/88382648-e2047780-cd76-11ea-9269-3def0ae746ce.png)

Repeat these steps for the `Microsoft.ML.OnnxRuntime` version `1.4.0` and `Microsoft.ML.ImageAnalytics` version `1.5.1`.

Alternately if you prefer working from the command line, you can run this command from the *src/Web* folder:

```dotnetcli
dotnet add package Microsoft.ML.OnnxTransformer -v 1.5.1
dotnet add package Microsoft.ML.OnnxRuntime -v 1.4.0
dotnet add package Microsoft.ML.ImageAnalytics -v 1.5.1
```

### Define model output schema

In the `Shared` project, add a new class called `ONNXOutput` to the root directory.

Start by adding the following using statement:

```csharp
using Microsoft.ML.Data;
```

Then, define the class as follows:

```csharp
public class ONNXOutput
{
    [ColumnName("detected_boxes")]
    public float[] DetectedBoxes { get; set; }

    [ColumnName("detected_scores")]
    public float[] DetectedScores { get; set; }

    [ColumnName("detected_classes")]
    public long[] DetectedClasses{ get; set; }
}
```

![Define ML.NET ONNX scoring pipeline output schema](https://user-images.githubusercontent.com/46974588/88382957-8981aa00-cd77-11ea-81fa-b4324b11bf03.png)

### Configure PredictionEnginePool service

The `PredictionEnginePool` is designed for use with dependency injection which is built into ASP.NET Core. As such, you configure it just like you would any other service you want to use throughout your application.

In the `Web` project, open the *Startup.cs* file.

Then, in the `ConfigureServices` method, register a `PredictionEnginePool` service. Give it a unique and descriptive name so that you are able to differentiate it from other models and provide the path where you saved your pipeline to.

```csharp
services.AddPredictionEnginePool<ONNXInput, ONNXOutput>().FromFile(modelName:"DamageDetection",filePath:@"C:/Dev/ONNXModel.zip");
```

![Configure damage detection ONNX prediction engine pool](https://user-images.githubusercontent.com/46974588/88383079-ccdc1880-cd77-11ea-9ff8-6e95f4c97193.png)

### Parse model outputs

The current pipeline takes an image and uses the ONNX model to detect damage in a vehicle. However, the outputs of the model still need to be parsed into bounding box objects. To do so, create a service in the web application.

#### Define bounding box schema

Start off by defining the bounding box schema.

In the *src/Web/Models* directory, create a new class called `BoundingBox` and add the following using statements at the top.

```csharp
using System.Drawing;
```

Then, replace the class definition with the following code:

```csharp
public class BoundingBox
{
    public Dimensions BoxDimensions { get; set; }
    public float Confidence { get; set; }
    public long DamageCategory { get; set; }
}

public class Dimensions
{
    public PointF P1 { get; set; }
    public PointF P2 { get; set; }
    public float Width { get { return P2.X - P1.X; } }
    public float Height { get { return P2.Y - P1.Y; } }
}
```

![BoundingBox model output mapping schema](https://user-images.githubusercontent.com/46974588/88383329-4bd15100-cd78-11ea-8f29-468544d86588.png)

The outputs form the model, especially the bounding boxes detected have to be mapped to a set of coordinates. The `detected_boxes` column provides the `x` and `y` coordinates for the top-left and bottom-right points of the bounding box. These values are represented as a float because they determine the percentage offset from the origin or top-left corner of the image.

The `Dimensions` class contains the coordinates of the top-left(`P1`) and bottom-right (`P2`) points. Using these points, we can calculate the width and height.

The `BoundingBox` class encapsulates the dimensions of the bounding box as well as how confident the model is that the bounding box is the assigned damage category.

#### Define damage detection service

Now that you have define the bounding box, it's time to add the service that will detect the damage in images using the model. 

In the *src/Web/Services*, create a new interface called `IDamageDetectionService` and add the following `using` statements at the top of the file.

```csharp
using System.Collections.Generic;
using Web.Models;
using Shared;
```

Replace the interface definition with the following code:

```csharp
public interface IDamageDetectionService
{
    IEnumerable<BoundingBox> DetectDamage(ONNXInput input, float threshold);
    string AnnotateBase64Image(string imagePath, IEnumerable<BoundingBox> boundingBoxes);
    float CalculateDamageTotalCost(IEnumerable<BoundingBox> boundingBoxes);
}
```

![IDamageDetectionService interface definition](https://user-images.githubusercontent.com/46974588/88383614-d5811e80-cd78-11ea-8798-fb22f9e8bc07.png)

This interface defines three methods to help with detecting damage in images:

- *DetectDamage*: Returns a list of `BoundingBox` objects with confidence greater-than or equal to the provided threshold from the outputs of the registered `DamageDetection` `PredictionEnginePool`.
- *AnnotateBase64Image*: Draws the detected bounding boxes over the provided image.
- *CalculateDamageTotalCost*: Provides the total cost of the damage found on the vehicle.

Now that you have a definition for the service, create a new class called `DamageDetectionService` in the *src/Web/Services* directory to implement the interface.

Add the following `using` statements at the top of the file:

```csharp
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Drawing;
using System.Drawing.Drawing2D;
using System.Drawing.Imaging;
using Microsoft.Extensions.ML;
using Web.Models;
using Shared;
```

Replace the class definition with the following code:

```csharp
public class DamageDetectionService : IDamageDetectionService
{
    private readonly PredictionEnginePool<ONNXInput, ONNXOutput> _damagePredictionEnginePool;

    public DamageDetectionService(PredictionEnginePool<ONNXInput, ONNXOutput> damagePredictionEnginePool)
    {
        _damagePredictionEnginePool = damagePredictionEnginePool;
    }
}
```

![DamageDetectionService implementation class definition](https://user-images.githubusercontent.com/46974588/88400290-218e8c00-cd96-11ea-83b0-bf86a0f08dbf.png)

The constructor takes the `PredictionEnginePool` service for the `DamageDetection` ML.NET pipeline.

Below the constructor, implement the `DetectDamage` method. The `DetectDamage` method maps uses the ONNX pipeline to detect damages on the image provided. Then, the generated `ONNXOutput` is mapped to a list of `BoundingBox` objects.

```csharp
public IEnumerable<BoundingBox> DetectDamage(ONNXInput input, float threshold)
{
    // Use prediction engine to make predictions
    ONNXOutput modelOutput = _damagePredictionEnginePool.Predict(modelName: "DamageDetection", example: input);

    // Map detected_boxes, detected_scores, detected_classes to BoundingBox objects
    var boxes = modelOutput.DetectedBoxes;
    var boundingBoxes = new List<BoundingBox>();
    for (var i = 0; i < boxes.Length; i += 4)
    {
        var boxIdx = i / 4;
        var boundingBox = new BoundingBox
        {
            BoxDimensions = new Dimensions { P1 = new PointF(boxes[i], boxes[i + 1]), P2 = new PointF(boxes[i + 2], boxes[i + 3]) },
            Confidence = modelOutput.DetectedScores[boxIdx],
            DamageCategory = modelOutput.DetectedClasses[boxIdx]
        };
        boundingBoxes.Add(boundingBox);
    }

    // Return bounding boxes that are at or above the threshold
    var topBoundingBoxes = boundingBoxes.Where(box => box.Confidence >= threshold).OrderByDescending(box => box.Confidence);
    return topBoundingBoxes;
}
```

![Implement DetectDamage method](https://user-images.githubusercontent.com/46974588/88400449-5ef31980-cd96-11ea-8930-20c2b1c60ea5.png)

Then, implement the `AnnotateBase64Image` method. This method takes in an image path and the bounding boxes detected and draws them onto the uploaded image. The annotated image is then encoded as a base64 string and returned to  the user.

```csharp
public string AnnotateBase64Image(string imagePath, IEnumerable<BoundingBox> boundingBoxes)
{
    using(var image = Image.FromFile(imagePath))
    {
        var originalImageWidth = image.Width;
        var originalImageHeight = image.Height;

        foreach (var box in boundingBoxes)
        {
            var left = (int)Math.Max(box.BoxDimensions.P1.X * originalImageWidth, 0);
            var top = (int)Math.Max(box.BoxDimensions.P1.Y * originalImageHeight, 0);
            var width = (int)Math.Min(originalImageWidth - left, box.BoxDimensions.Width * originalImageWidth);
            var height = (int)Math.Min(originalImageHeight - top, box.BoxDimensions.Height * originalImageHeight);

            using (Graphics thumbnailGraphic = Graphics.FromImage(image))
            {
                thumbnailGraphic.CompositingQuality = CompositingQuality.HighQuality;
                thumbnailGraphic.SmoothingMode = SmoothingMode.HighQuality;
                thumbnailGraphic.InterpolationMode = InterpolationMode.HighQualityBicubic;

                Pen pen = new Pen(Color.Red, 3.2f);

                // Draw bounding box on image
                thumbnailGraphic.DrawRectangle(pen, left, top, width, height);
            }
        }

        using (var ms = new MemoryStream())
        {
            image.Save(ms, ImageFormat.Jpeg);
            var base64Image = Convert.ToBase64String(ms.ToArray());
            return $"data:image/jpg;base64,{base64Image}";
        }
    }
}
```

![AnnotateBase64Image method implementation](https://user-images.githubusercontent.com/46974588/88400527-7d591500-cd96-11ea-8a03-67b7fac77a6f.png)

Finally, implement the `CalculateDamageTotalCost` method:

```csharp
public float CalculateDamageTotalCost(IEnumerable<BoundingBox> boundingBoxes)
{
    float total = 0;

    if (boundingBoxes.Count() == 0) return total;

    foreach (var boundingBox in boundingBoxes)
    {
        switch (boundingBox.DamageCategory)
        {

            case 0L: // Cracked windshield
                total += 200;
                break;
            case 1L: // Dent
                total += 100;
                break;
            default:
                break;
        }
    }
    return total;
}
```

![CalculateDamageTotalCost method implementation](https://user-images.githubusercontent.com/46974588/88400624-9c57a700-cd96-11ea-9f7c-712d184ab2d6.png)

If the car has a cracked windshield, the cost of the vehicle drops by $200. If the car has a dent, then the price drops by $100.

#### Register damage detection service

Now that you have implemented the damage detection service, you have to register it in the *Startup.cs* file so it is available throught your application.

Add the following code at the bottom of `ConfigureServices` method of the *Startup.cs* file of the `Web` project:

```csharp
services.AddTransient<IDamageDetectionService, DamageDetectionService>();
```

![Register damage detection service in Startup](https://user-images.githubusercontent.com/46974588/88385665-c4d2a780-cd7c-11ea-8687-49aeb4c2bd1a.png)

#### Use damage detection service

Use the `DamageDetectionService` to detect damage and adjust the price of your car accordingly.

Open the *src/Web/Pages/Index.cshtml.cs* file and directly below the `_carModelService` variable definition add the following code:

```csharp
private readonly IDamageDetectionService _damageDetectionService;
```

Replace the constructor with the following code which injects and sets the value of  `_damageDetectionService`.

```csharp
public IndexModel(IWebHostEnvironment env, ILogger<IndexModel> logger, ICarModelService carFileModelService, PredictionEnginePool<ModelInput, ModelOutput> pricePredictionEnginePool, IDamageDetectionService damageDetectionService)
{
    _env = env;
    _logger = logger;
    _carModelService = carFileModelService.GetDetails();
    CarMakeSL = new SelectList(_carModelService, "Id", "Model", default, "Make");
    _pricePredictionEnginePool = pricePredictionEnginePool;
    _damageDetectionService = damageDetectionService;
}
```

![Inject IDamageDetectionService](https://user-images.githubusercontent.com/46974588/88400827-e2ad0600-cd96-11ea-879d-a621bab11099.png)

Replace the `ProcessUploadImageAsync` method with the following code which uses the `IDamageDetection` service to get the bounding boxes and annotate the image.

```csharp
private async Task<IEnumerable<BoundingBox>> ProcessUploadedImageAsync(IFormFile uploadedImage)
{
    // Save uploaded image
    var fileName = Path.Combine(_env.ContentRootPath, "ImageTemp", $"{Guid.NewGuid().ToString()}.jpg");
    using (var fs = new FileStream(fileName, FileMode.Create))
    {
        //Copy image to memory stream
        await uploadedImage.CopyToAsync(fs);
    }

    // Create ONNX input
    var imageInput = new ONNXInput
    {
        ImagePath = fileName
    };

    // Detect damage
    var boundingBoxes = _damageDetectionService.DetectDamage(imageInput, 0.7f);
    CarInfo.Base64Image = _damageDetectionService.AnnotateBase64Image(fileName, boundingBoxes);

    return boundingBoxes;
}
```

![New ProcessUploadedImageAsync method](https://user-images.githubusercontent.com/46974588/88401001-2738a180-cd97-11ea-8ac6-6db29cfe01bb.png)

The `ProcessUploadedImageAsync` method takes the uploaded image saves it to the `ImageTemp` directory in the `Web` project. Once the image is saved, the `IDamageDetectionService` is used to generate the bounding boxes (if any) and annotate the image with them. The annotated image is then encoded as a base64 string for display on the web page. Finally, the bounding boxes are returned for further processing.

Replace the `OnPost` method with the following code for the `OnPostAsync` method:

```csharp
public async Task OnPostAsync()
{
    var selectedMakeModel = _carModelService.Where(x => CarModelDetailId == x.Id).FirstOrDefault();

    CarInfo.Make = selectedMakeModel.Make;
    CarInfo.Model = selectedMakeModel.Model;

    ModelInput input = new ModelInput
    {
        Year = (float)CarInfo.Year,
        Mileage = (float)CarInfo.Mileage,
        Make = CarInfo.Make,
        Model = CarInfo.Model
    };

    ModelOutput prediction = _pricePredictionEnginePool.Predict(modelName: "PricePrediction", example: input);

    CarInfo.Price = prediction.Score;

    if (ImageUpload != null)
    {
        var boundingBoxes = await ProcessUploadedImageAsync(ImageUpload);
        var damageCost = _damageDetectionService.CalculateDamageTotalCost(boundingBoxes);
        CarInfo.Price = Math.Max(0,CarInfo.Price - damageCost);
        ShowImage = true;
    }
    ShowPrice = true;
}
```

![Replace OnPostAsync method](https://user-images.githubusercontent.com/46974588/88401253-7da5e000-cd97-11ea-90d8-0a278baa9836.png)

In the snippet above, the information from the `CarInfo` model is taken and a new instance of `ModelInput` is created. Then, the `Predict` function is used to predict the price of a vehicle, given the inputs from the `input` variable. Once a prediction is made, the `Price` property of the `CarInfo` is set to the predicted value.

As an additional step, if an image is uploaded, the `IDamageDetection` service is used to adjust the price of the vehicle if any damage is found.

Then, the price of the car is displayed on screen.

### Run the app

Set the startup project to `Web` and run the application. Fill in the form fields and select **Predict Price**.

Compared with the original application which did not use object detection, we can see that the original price of $13806 is reduced to $13706 because of the dent.

**No object detection**
![Consume price prediction model in web app](https://user-images.githubusercontent.com/46974588/88406656-f9575b00-cd9e-11ea-9481-0c574822e950.png)

**Object detection**
![Consume both ONNX and price prediction models in web app](https://user-images.githubusercontent.com/46974588/88401414-b940aa00-cd97-11ea-9388-468b024e733f.png)



Congratulations! You have now used both the price prediction and object detection ONNX models inside your web application.