# Phase 11: ONNX

In this session, you will learn how to use an object detection ONNX model from Azure Custom Vision inside a .NET application.

## What is Azure Custom Vision?

Azure Custom Vision is a cloud-based service that lets you build, deploy, and export custom deep learning computer vision models. Visit the [Azure Custom Vision](https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/home) documentation to learn more.

## What is ONNX?

The Open Neural Network Exchange (ONNX) is an open source format for AI models. ONNX supports interoperability between frameworks. This means you can train a model in one of the many popular machine learning frameworks like PyTorch, convert it into ONNX format and consume the ONNX model in a different framework like ML.NET. To learn more, visit the [ONNX website](https://onnx.ai/).

![ONNX Description](https://user-images.githubusercontent.com/46974588/88130578-eaa75300-cba8-11ea-9da6-b1fd90169f8a.png)

## Phase 11.1: Inspect the model

Download the ONNX object detection model and save it anywhere on your computer.

In your browser, open [Netron](https://lutzroeder.github.io/netron/).

In Netron, select **Open Model...** and use the file explorer to import the ONNX model.

A graph appears showing the layers and nodes of your model. Nodes have annotations and metadata which describes the model such as the model format, a short description, inputs and outputs. 

![Netron Azure Custom Vision Object Detection](https://user-images.githubusercontent.com/46974588/88125411-3c49e080-cb9d-11ea-80d8-661bc85cf594.png)

Select the first node in the graph, labeled **data**.

The input for the model is the following:

- *data*: A `float` array of size 3 x 320 x 320 containing the RGB values of a 320px x 320px image.

The output generated by the model contains the following:

![ONNX Azure Custom Vision Outputs](https://user-images.githubusercontent.com/46974588/88129943-525c9e80-cba7-11ea-986d-016cee6b9b9b.png)

- *detected_boxes*: a `float` array of size `n x 4` where `n` is the number of bounding boxes found in the image. The 4 represents the coordinates of two points P1(x1,y1) and P2(x2,y2) for each bounding box found by the model. The first point corresponds to the top-left point of the bounding box and the second point corresponds to the bottom-right point of the bounding box. 

    ![ONNX Azure Custom Vision Detected Boxes](https://user-images.githubusercontent.com/46974588/88129961-57b9e900-cba7-11ea-9b3d-5339336ad851.png)

- *detected_scores*: a `float` array of size `n` where `n` is the number of bounding boxes found in the image.
- *dectected_classes*: a `float` array of size `n` where `n` is the number of bounding boxes found in the image. The model used in this sample detects two classes:
  - 0: CrackedWindshield
  - 1: Dent

## 11.1: Install NuGet packages

First, we need to add a few NuGet packages to the `ONNXConsole` project. 

If you're using Visual Studio, right click on the project name and select **Manage NuGet Dependencies**. Then click the "Browse" tab and search for `Microsoft.ML.OnnxTransformer`. Make sure to install version **1.5.1**.

Alternately if you prefer working from the command line, you can run this command from the *src/Shared* folder:

```dotnetcli
dotnet add package Microsoft.ML.OnnxTransformer -v 1.5.1
```

Repeat these steps for the `Microsoft.ML.OnnxRuntime` version `1.4.0` and `Microsoft.ML.ImageAnalytics` version `1.5.1`.

### Create Input Schema

Open the `Shared` project and create a new class called `ONNXInput.`

Replace the class definition with the following code:

```csharp
public class ONNXInput
{
    public string ImagePath { get; set; }
}
```

## 11.2: Create pipeline

Open the `Program.cs` file in the `ONNXConsole`.

Add the following `using` statements:

```csharp
using Microsoft.ML;
using Shared;
```

Inside the `Main` method, initialize the the `MLContext`.

```csharp
MLContext mlContext = new MLContext();
```

Create a pipeline to load an image and uses the ONNX model to detect objects in the image.

Below the `mlContext`, add the following code:

```csharp
var pipeline =
    mlContext.Transforms.LoadImages("Image", null, "ImagePath")
    .Append(mlContext.Transforms.ResizeImages("ResizedImage", 320, 320, "Image"))
    .Append(mlContext.Transforms.ExtractPixels("data", "ResizedImage"))
    .Append(mlContext.Transforms.ApplyOnnxModel(
        outputColumnNames: new string[] { "detected_boxes", "detected_scores", "detected_classes" },
        inputColumnNames: new string[] { "data" },
        modelFile: @"C:/Users/luquinta.REDMOND/Downloads/BriCompact.ONNX/model.onnx"));
```

This pipeline performs the following steps:

- `LoadImages` takes the path of an image and creates a Bitmap in the `Image` column.
- The model expects a 320px x 320px image. Therefore, the contents of the `Image` column are resized and stored in the `ResizedImage` column.
- Once the image is resized, extract the pixels into a column called `data` using the `ExtractPixels` transform. Notice that the name of the column containing the pixels is `data` because that's the entrypoint of the model.
- Finally, the model is used to detect objects by using `ApplyOnnxModel` transform.

## 11.3: Create ML.NET model from pipeline

Now that you have the pipeline, you can create an ML.NET model that contains all of the preprocessing and scoring transforms. 

Since no training is actually taking place, you can use an empty `IDataView` when fitting the pipeline.

Below your pipeline definition, create an empty IDataView and call `Fit` to create the model.

```csharp
var emptyDV = mlContext.Data.LoadFromEnumerable(new ONNXInput[] { });
var model = pipeline.Fit(emptyIdv);
```

Now that you have a model, save it for later use.

```csharp
mlContext.Model.Save(model, emptyDV.Schema, @"C:/Dev/ONNXModel.zip");
```